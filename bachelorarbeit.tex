\documentclass[doc,a4paper,12pt]{apa6}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage[onehalfspacing]{setspace}

 
\title{Titel der Bachelorarbeit}
\shorttitle{Komplexe Regelerkennung}
\author{Carlo Michaelis}
\date{23. August 2013}
\affiliation{Universität Leipzig}
\abstract{Abstract}
\ccoppy{Creative Commons ...}

\begin{document}

%\maketitle
%\newpage

%\tableofcontents
%\newpage

\section{Einleitung}

Viele Fragen sind bis heute ungeklärt und werden es wohl auch weiterhin bleiben. Trotzdem scheint es, als gäbe es – vor allem bei der Erklärung unserer physikalischen Umgebung – schon einige befriedigende Antworten, die nicht nur die notwendige Funktionalität ermöglichen, sondern Großteils bereits unseren Hunger nach Verständnis decken. In einem Bereich jedoch, dem Bereich psychischer Prozesse, gibt es zwar bereits viele Einzelerkenntnisse, eine umfassende Theorie über die psychischen Funktionsweisen komplexerer Organismen steht jedoch aus. Vor allem das Zusammenspiel, also die Schnittstelle zwischen der Psyche und dem Körper, die Frage nach Determiniertheit und Selbstbestimmung, nach Automatismus und Veränderbarkeit sind bei weitem noch nicht ganzheitlich geklärt.\\
Dabei erfüllt das Verständnis darüber nicht nur den Zweck Wissen über Lebewesen zu akkumulieren oder einige neugierige Personen zu befriedigen, vielmehr prägt die Erkenntnis über die Funktionsweise unsere Gesellschaft, sie entscheidet u.a. über Gesetze, Konventionen und Kultur.

So sehr wir uns als Menschen den „freien Willen“ auch wünschen , also durch den Geist gezielt und indeterminiert unsere Welt verändern zu können, ohne unwillkürliche Automatismen wäre ein Leben auf der Erde wohl unmöglich. In vielen Situationen müssen innerhalb kürzester Zeit bestehende Reize erfasst, zukünftige Ereignisse simuliert und schließlich vorhergesagt werden.\\
Ein gutes Beispiel für die hohe Leistungsfähigkeit der automatisch ablaufenden Systeme des Menschen ist seine Sprache. Deutlich wird dies schon allein durch das Segmentationsproblem beim Sprachverstehen. Mit physikalischen Mitteln aufgezeichnete Frequenzen von Schallwellen zeigen Muster, die bei visueller Betrachtung kaum korrekt in Worte getrennt werden können. Eine Pause oder eine Frequenzschwankung sagt nur selten tatsächlich auch den Beginn eines neuen Wortes vorher. Zur Segmentation der Worte werden bei weitem nicht nur die physikalischen Signale, sondern zusätzlich viele verschiedene Hinweisreize benötigt (u.a. Saffran, Newport \& Aslin, 1996; Brent \& Cartwright, 1996). Die Segmentation muss jedoch nicht nur anhand einer großen Menge an Informationen, sondern auch innerhalb kürzester Zeit erfolgen, die ohne eine automatisierte Vorhersage wohl kaum möglich wäre.\\
Beim Sprechen wiederum ist zum Beispiel das so genannte Speech Monitoring (Levelt, 1983; Postma, 2000) von hoher Bedeutung. Das bezeichnet die Fähigkeit die inhaltliche Qualität des gesprochenen noch während dem Sprechen reflektieren zu können. Das bedeutet wiederum, dass beim Sprechen mindestens zwei Prozesse gleichzeitig ablaufen. Das legt nah, dass hier ein hoher Automatismus-Grad im Spiel ist. In einer Studie von Goldmann-Eisler (1958) konnte gezeigt werden, dass u.a. immer dann größere Sprechpausen entstehen, wenn kommende Wörter schwerer vorhersagbar sind. Umgekehrt scheint Sprache also auch entscheidend von der Fähigkeit zur Vorhersage abhängig zu sein.\\
Bezüglich der auditiven Komponente sei vor allem auch die Musik hervorgehoben. Gerade in der Musik spiegelt sich die Tendenz des Menschen zu erfolgreichen Vorhersagen wieder. Drake und Bertrand (2001) prüften 5 universelle, d.h. nicht-kulturabhängige Paradigmen, wie sich Menschen zu Musik verhalten. In Paradigma 3 beschrieben sie, dass sich Menschen, die im Takt der Musik bleiben sollen, in mehr als 90\% der Zeit erfolgreich mit Musik synchronisieren können. D.h. sie können Vorhersagen erfolgreich treffen und ihre Handlung auch entsprechend erfolgreich an den Vorhersagen steuern. Man könnte mutmaßen, dass genau in der Vorhersagbarkeit der Reiz der Musik liegt und auch ein Harmonieempfinden damit zusammenhängt.

Die Bedeutung der Vorhersagbarkeit und vor allem der diesbezüglich wirkenden Automatismen lässt sich an den vorigen Beispielen recht gut erkennen. Im folgenden Experiment soll es um die Vorhersagbarkeit von komplexen Regeln in auditiven Stimuli gehen. Konkret soll untersucht werden inwiefern komplexe Regeln von Tonabfolgen (wie sie etwa auch in der Sprache vorkommen) erkannt und insbesondere vorhergesagt bzw. Abweichungen erkannt werden können. Im Folgenden soll zunächst der Forschungsstand bezüglich der auditiven Regelerkennung erläutert werden. Anschließend wird eine kurze Einführung in Lerntheorien erfolgen, die speziell für dieses Experiment von Bedeutung sein wird.

\subsection{Missmatch Negativity (MMN) und das Oddball Paradigma}

Die Missmatch Negativity (MMN) ist ein ereigniskorreliertes Potential (EKP). Bei ereigniskorrelierten Potentialen handelt es sich um elektrische Potentiale des Gehirns, welche zu einem bestimmten Zeitpunkt (Ereignis) eintreten und mit diesem Zusammenhängen und daher als ereigniskorreliert betrachtet werden können. Die Messung dieser Signale erfolgt per Elektroenzephalogramm (EEG), welches elektrische Potentiale des Gehirns erfassen kann. Um die Signale des Ereignisses letztlich vom Rauschen anderer Gehirnpotentiale trennen zu können, werden die Potentiale über mehrere Messungen gemittelt.

Erstmals wurde die MMN von Näätänen, Gaillard \& Mäntysalo (1978) für auditive Stimuli beschrieben. Verwendet wurde das so genannte Oddball Paradigma, bei dem eine Reihe gleicher Stimuli gegeben wird, welche dann von einem nicht-regelkonformen Stimulus unterbrochen wird. Die MMN trifft dann ca. 150ms – 250ms nach der Präsentation des Abweichers vor allem im fronto-zentralen Bereich auf. Sie zeigt damit an, dass eine Vorhersage (bzw. eine Extrapolation) des Gehirns fehlerhaft ist. Im Umkehrschluss lässt sich bei einem Auftreten der MMN schließen, dass das Gehirn zuvor eine Regelmäßigkeit erkannt haben muss. Diese Ableitung wurde auch in den Studien verwendet, die diesem Experiment zugrunde liegt.

Eine weitere Eigenschaft der MMN ist, dass sie auch dann auftritt, wenn der auditive Stimulus nicht beachtet wird. Z.B. wenn Probanden während dem Hören der Töne einen Stummfilm schauen.\\
Das Gehirn scheint also keine (oder nur sehr wenige) Aufmerksamkeitsressourcen für die Erkennung einer Regelmäßigkeit in einem auditiven Stimulus zu benötigen und die Verarbeitung erfolgt damit relativ automatisch.

\subsection{Komplexe Regelerkennung}

Es stellte sich schon bald die Frage, ob die MMN auch bei deutlich komplexeren Regeln auftritt. Dies würde – oben bereits genannt – implizieren, dass das Gehirn die Fähigkeit besitzt komplexe Regeln zu extrahieren und vorhersagen zu können.\\
Die ersten Paradigmen zur Untersuchung komplexerer Regeln bei auditiven Stimuli ergaben, dass die Regeln zwar vom Gehirn erkannt (gemessen per MMN), jedoch nicht bewusst verarbeitet werden konnten. Im Sequence Learning Paradigm (Hoffmann \& Koch, 1998) sollten die Probanden ein sich wiederholendes Muster erkennen, welches in einer Ton-Sequenz versteckt war, im Covariation Detection Paradigm (Stamov Rossnagel 2001) wiederum lernten die Probanden nichtsaliente Verbindungen zwischen den einzelnen Stimulus-Elementen. Obwohl die Probanden die Regeln meist nicht bewusst erkannten, zeigten sich durch Übung Leistungssteigerungen in Form von zunehmend erhöhter Geschwindigkeit und erhöhter Genauigkeit, was zunächst auf implizites Lernen hinweist.

Paavilainen, Arajärvi \& Takegata (2006) warfen daraufhin die Frage auf in welchem Ausmaß der Erkennungsprozess Aufmerksamkeitsressourcen benötigt. Dabei sollte vor allem geklärt werden, ob die Regelerkennung komplexerer Regeln eventuell bereits präattentiv auf der Ebene des sensorischen Gedächtnisses stattfindet. Um dies zu untersuchen wurden den Probanden je 2000 Töne vorgespielt, die sich in ihrer Länge (50ms oder 150ms) und in ihrer Höhe (1000Hz oder 1500Hz) unterschieden. Dabei konnten zwei gegensätzliche Regeln vorkommen:

\begin{APAitemize}
  \item Auf einen langen Ton folgt ein hoher Ton, auf einen kurzen Ton folgt ein tiefer Ton
  \item Auf einen langen Ton folgt ein tiefer Ton, auf einen kurzen Ton folgt ein hoher Ton
\end{APAitemize}

Von den zwei Merkmalen (Höhe und Länge) ist jeweils die Länge der entscheidende Prädiktor für das Merkmal Höhe des nächsten Tons. Eine Beispielsequenz dieser Regel ist in Abbildung 1  im Abschnitt Methoden zu sehen.\\
Alle Probanden durchliefen in gegebener Reihenfolge folgende Bedingungen:

\begin{APAitemize}
  \item Ignore: Die Probanden schauten einen Stummfilm mit Untertiteln und sollten nicht auf die Töne reagieren.
  \item Implicit detection: Den Probanden wurde die Regel nicht erklärt, sie sollten jedoch eine Taste drücken wenn sie das Gefühl hätten, dass ein Ton nicht passt.
  \item Explicit detection: Den Probanden wurde die Regel erklärt (mit Hilfe eines Schaubildes), sie sollten eine Taste drücken, wenn der letzte Ton von der Regel abweicht.
\end{APAitemize}

Im Ergebnis zeigte sich, dass in allen drei Bedingungen der Ausschlag der MMN nicht signifikant voneinander abweicht. Es war also unerheblich, ob die Probanden den Ton ignorierten oder aktiv darauf achteten. Auch implizite, so wie explizite Lernprozesse waren demnach anscheinend nicht vorhanden. Für einen – zumindest kleinen – Lerneffekt spricht hingegen, dass die behavioralen Daten in den zwei detection-Bedingunen in Form der Erfolgsrate beim Drücken der Tasten signifikant vom Zufall abwichen, wenn auch nur sehr gering. Qualitativ wurde außerdem berichtet, dass die Probanden das Wissen über die Regel nur als wenig hilfreich einschätzten.\\
Aus den Ergebnissen wurde geschlossen, dass die Verarbeitung wohl überwiegend automatisiert und die Treffer in der explicit detection Bedingung eher auf Intuition, als auf explizites Wissen über die Regel zurückzuführen ist. Allerdings scheinen die neuronalen Mechanismen, die auch die MMN auslösen ihre Leistungsfähigkeit verbessern zu können. Dies schlägt sich zumindest in der knapp überzufälligen Erfolgsrate nieder. Letztlich war ein weiteres EKP, die so genannte P300, nicht ausgeprägt. Diese Komponente schlägt üblicherweise aus, wenn Stimuli bewusst klassifiziert werden. Die These, dass die Stimuli automatisiert und unbewusst verarbeitet werden konnte auch damit bestätigt werden.

Die Studie von Paavilainen, Arajärvi \& Takegata (2006) wurde 2 Jahre später von Bendixen, Prinz, Horváth, Trukillo-Barreto \& Schröger (2008) repliziert und erweitert. Dabei wurden die gleichen Regeln verwendet, auch Tonhöhe und Frequenz entsprach den vorigen.\\
Die Regeln wurden in unterschiedlich langen Sequenzen (4, 9, 14, 19 Töne) regelkonform präsentiert und anschließend durch irreguläre Sequenzen (4-8 Töne) unterbrochen. Diese Unterbrechung führt dazu, dass das Gehirn immer wieder von vorne beginnen muss eine Regel zu erkennen, zumal nach einer irregulären Sequenz auch die reguläre Sequenz variieren kann, da wie oben beschrieben, zwei solcher regulären Regeln existieren. Es wurde zwischen zwei Bedingungen unterschieden:

\begin{APAitemize}
  \item Passiv: Kontinuierliche Stimuli-Präsentation, wobei dir Probanden einen Stummfilm schauten und Stimuli ignorieren sollten.
  \item Aktiv: Probanden hörten abgegrenzte Sequenzen (6, 11, 16, 21 Töne), wobei letzter Ton Standard oder Deviant sein sollte. Sie bekamen die Regeln erklärt und sollten per Tastendruck reagieren.
\end{APAitemize}

Die Probanden bekamen erst 8 Blöcke in der Passiv-Bedingung präsentiert, wurden gefragt, ob sie eine Regel erkannten, anschließend bekamen sie 5 weitere Blöcke in der Aktiv-Bedingung präsentiert. In keinem Fall erkannten die Probanden die Regel nach der Passiv-Bedingung korrekt, sofern sie überhaupt eine erkannten. Die Sensitivität (d-prime) lag noch signifikant über dem Zufallsniveau, sowohl für kurze (5 und 10 Töne), als auch für lange (15 und 20 Töne) Sequenzen. Die MMN zeigte jedoch nur für lange Sequenzen eine signifikante Ausprägung.\\
Die Ergebnisse von Paavilainen et al. konnten damit zum einen bestätigt werden, zum anderen konnte gezeigt werden, dass für die Regelerkennung zunächst einige Stimuli nötig sind, bis das Gehirn die entsprechende Regel abgeleitet hat. Allgemein liegt ein kontinuierlicher Zusammenhang vor. Je mehr regelkonforme Stimuli präsentiert werden, desto höher ist der Ausschlag der MMN. Es scheint also eine Akkumulation von Hinweisen auf eine Regel vorzuliegen. Entscheidend ist jedoch, dass auch in dieser Studie das bewusste Erkennen einer Regelverletzung nicht möglich war. Auch hier zeigte sich die Abwesenheit der P300 -Komponente und damit die Abwesenheit bewusster Klassifikationsprozesse.

\subsection{Lernen prozeduralen Wissens}

Zusammenfassend liegt also bezüglich der Erkennung komplexer Regeln eine automatisierte und unbewusste Verarbeitung vor. Für eine weitere Untersuchung ist es zunächst von Bedeutung zu betrachten wie das implizite und explizite prozedurale Lernen funktioniert. Daraus könnte abgeleitet werden wie ein expliziter prozeduraler Lernprozess komplexer Regeln gestaltet werden könnte. Sollte dieser erfolgreich sein könnte dies ein Ansatz sein um die Funktionsmechanismen hinter der Lücke zwischen der automatisierten unbewussten und der manuellen bewussten Verarbeitung zu verstehen.

Grundsätzlich lässt sich der Lernprozess recht eindeutig als prozedurales Lernen (abgegrenzt von deklarativem Lernen) klassifizieren, sofern das Drücken der Taste passend zu den Abweichlern gelernt wird. Außerdem lag in den bisherigen Untersuchungen – wenn überhaupt – nur implizites Lernen vor. Interessant wäre also zu betrachten wie explizites Lernen erreicht werden kann, welches bisher den Probanden nicht möglich war. Prinzipiell ist auf Grund der genannten Ergebnisse vermutlich davon auszugehen, dass sich ein expliziter Lernprozess relativ schwierig gestalten könnte.

Die Klassifikation in prozedurale Prozesse macht nun deutlich, dass ein einfaches Zeigen der Regel (z.B. an einem Schaubild) kaum ausreichen kann, um die Regel adäquat anzuwenden. Zumal die Stimuli mit 50ms bzw. 150ms Länge und einem Inter-Stimulus-Intervall (ISI) von 300ms präsentiert wurden. Diese hohe Geschwindigkeit erfordert wahrscheinlich ein langes Training, sofern ein expliziter Prozess verlangt wird.

Fitts \& Posner (1967) gingen davon aus, dass prozedurales Wissen über drei Schritte hintereinander gelernt werden kann:

\begin{APAenumerate}
  \item Kognitives Stadium: In diesem Stadium handelt es sich noch um eine deklarative Repräsentation. Das deklarative Wissen kann in bestehendes Wissen integriert und mit diesem verknüpft werden. Eine Vermittlung sollte präzise und verständlich erfolgen. Angewendet werden kann z.B. ein Schaubild.
  \item Assoziatives Stadium: Eine prozedurale Repräsentation wird aufgebaut. Es entwickeln sich Wenn-Dann-Regeln. Sofern ein bestimmtes Ereignis eintritt wird automatisch eine spezifische Reaktion ausgeführt. Dieses Stadium kann vor allem durch regelmäßige Übung erreicht werden. Wichtig ist hier auch das Geben von Feedback.
  \item Autonomes Stadium: Das prozedurale Wissen kann schnell und automatisiert abgerufen werden. Die einzelnen Repräsentationen sind stabil miteinander verbunden und führen zu einer fehlerfreien Ausführung. Der Ressourcenaufwand minimiert sich. Erreicht wird dies durch das Praktizieren über längere Zeiträume.
\end{APAenumerate}

Eine gute Beschreibung des prozeduralen Lernens bietet auch der Learning Cycle von Withmore (2009). Der Kreislauf teilt das Lernen in 4 aufeinanderfolgende Schritte ein:

\begin{APAenumerate}
  \item Unconscious incompetence: Kein Verständnis vorhanden
  \item Conscious incompetence: Es liegt zwar nur eine geringe Performanz vor, Fehler und Schwächen werden jedoch bemerkt und können korrigiert werden
  \item Conscious competence: Die Leistung verbessert sich, zur Durchführung ist jedoch ein hoher kognitiver Aufwand nötig
  \item Unconscious competence: Es liegt eine hohe Performanz vor, der kognitive Aufwand verringert sich deutlich zugunsten automatisierter Abläufe
\end{APAenumerate}

Warum ein explizites Lernen in den vorigen Studien wahrscheinlich nicht möglich war kann nun auf Basis der beiden Modelle sehr schnell erklärt werden. Unter der Voraussetzung, dass prozedurales Wissen vorliegt und gelernt werden muss wurde in beiden Studien vermutlich lediglich das kognitive Stadium erreicht (nach Fitts \& Posner). Es wurden zwar Übungsdurchgänge vorgenommen, diese waren aber direkt in der Standard-Geschwindigkeit. Die Wahrscheinlichkeit, dass stabile Wenn-Dann-Regeln aufgebaut werden konnten ist sehr gering, da die Versuchspersonen vermutlich auf Grund der Überforderung keine Möglichkeit hat ihr Verhalten adäquat zu überdenken (fehlendes Feedback) und der Schwierigkeitsgrad zu abrupt ansteigt. Lernprozesse funktionieren am besten, wenn der Schwierigkeitsgrad langsam und kontinuierlich zunimmt.

% Hier dringend noch Studie suchen!!

Nach dem Modell von Withmore wird sogar gar nicht erst die zweite Stufe erreicht, da Fehler und Schwächen nicht bemerkt werden. Es dürfte demnach also keine Verbesserung der Leistung eintreten können.. Diese Erkenntnisse sollen im Folgenden verwendet werden.

\subsection{Hypothese}

Es soll an dieser Stelle zunächst noch einmal der aktuelle Stand zusammengefasst werden:
Lediglich Schaubild!
Verwendung Fitts \& Posners

Diskussion?
Sind die beiden Verarbeitungsmethoden unabhängig voneinander? (Evtl. erst Diskussion) Schnell unbewusst unbeinflussbar vs. langsam bewusst beeinflussbar?


\section{Methoden}

[...]

\end{document}